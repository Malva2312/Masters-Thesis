\chapter{State of the Art}\label{chap:sota}

\section{Lung Nodule Characterization}

The detection of lung nodules is only one of the first steps when the objective is an assertive diagnosis of lung cancer. Once identified, the characteristics of these nodules must be analyzed to establish their risk of malignancy. This process, known as lung nodule characterization, is critical to reducing false positives and guiding more effective clinical decisions.

When discussing characterization, analyzing various features extracted from CT scans is important. These features include the size and volume of the nodules, as larger nodules may be more suspicious~\cite{Alksas2023}. The shape and margins are also significant; benign nodules typically have more regular shapes, while malignant nodules often appear irregular, spiculated, or lobulated~\cite{Shaffie2022}. Other factors to consider are texture and density, as these can provide valuable insight into the nodule~\cite{Alksas2023,Shaffie2022}.

Recognizing that accurate diagnosis is crucial in preventing death from lung cancer, a traditional approach relies on visual assessment by radiologists. However, this method is often time-consuming and may lead to errors, especially when dealing with smaller nodules or those that have more subtle characteristics. The National Lung Screening Trial (NLST) has shown that low-dose CT screening is an effective way of fighting lung cancer, detecting tumors at early stages, and significantly reducing mortality~\cite{NLST}. On the other hand, it's also important to recognize that screening with low-dose CT can result in false-positive outcomes. There was a significantly higher rate of positive results in the low-dose CT group (24.2\%) compared to the X-ray group (6.9\%).  False-positive results in lung cancer screening can lead to invasive and unnecessary diagnostic procedures, which entails risks for patients and additional costs for healthcare systems.

Here is where we found space for new technological advances like CAD systems and other techniques for automated lung characterization, that may help radiologists to interpret images more efficiently and consistently, reducing human error.


\section{Datasets Overview}
This section presents some of the most well-known and widely used datasets in lung nodule characterization tasks.

\subsection{LIDC-IDRI }
The Lung Image Database Consortium Image Collection (LIDC-IDRI) is a comprehensive collection of CT scans of the thorax, designed for the diagnosis of lung cancer and the detection of visualized lesions. This internationally accessible database serves as a valuable resource for the development of computer-aided detection (CAD) systems focused on lung cancer diagnosis and evaluation. Launched by the National Cancer Institute (NCI) and further developed by the Foundation for the National Institutes of Health (FNIH), with support from the Food and Drug Administration (FDA), this public-private partnership exemplifies the success of a consensus-based consortium.

The creation of this data registry involved collaboration among seven academic research centers and eight major medical imaging companies, resulting in a total of 1,018 cases. Each case includes clinical thoracic CT scan images for individual subjects, accompanied by an XML file that details the results of a two-phase image annotation process. In the first phase, four radiologists independently reviewed CT images and annotated lesions into one of three categories: "nodule >=3 mm," "nodule <3 mm," and "non-nodule >=3 mm." During the second phase, the radiologists reviewed their annotations alongside the anonymized annotations of their peers to reach a consensus. This process was designed to allow for the accurate tallying of lung nodules on a CT scan with minimal human intervention, without requiring forced agreement among the radiologists~\cite{Armato2015}.

\subsection{LUNA16}
The Lung Nodule Analysis 2016 dataset utilizes the publicly available LIDC/IDRI database mentioned earlier. Scans with a slice thickness greater than 2.5 mm were excluded from the dataset. In total, there are 888 CT scans included. The reference standard for this challenge consists of all nodules that are 3 mm or larger, which were accepted by at least 3 out of 4 radiologists. Annotations that are not part of the reference standard, such as non-nodules, nodules smaller than 3 mm, and nodules annotated by only 1 or 2 radiologists, are classified as irrelevant findings~\cite{LUNA16}.

\subsection{NLST}
The National Lung Screening Trial, previously mentioned, was a randomized controlled trial conducted by the Lung Screening Study Group (LSS) and the American College of Radiology Imaging Network (ACRIN). The purpose of the trial was to evaluate whether screening for lung cancer with low-dose helical computed tomography (CT) reduces mortality compared to screening with chest radiography in high-risk individuals. Approximately 54,000 participants were enrolled between August 2002 and April 2004. Data collection for the study has concluded, with the final information gathered by December 31, 2009~\cite{NLST2013,NLST}.

\subsection{Limitations Acknowledge}

It is important to recognize that medical imaging datasets significantly contribute to developing and validating computer-aided detection (CAD) systems for lung cancer diagnosis. Nevertheless, even the most used datasets, such as LIDC-IDRI and LUNA16, have some limitations that can impact model performance and generalizability. These challenges include a lack of annotated data, subjectivity in labeling, inter-observer variability, and potential biases within the datasets. These factors complicate the creation of robust and broadly applicable models. Additionally, we acknowledge that the process of annotating medical images is both time-consuming and costly, often leading to datasets that are limited in size. This limitation is further exacerbated by patient data privacy regulations, which restrict access to larger datasets.~\cite{Gu2021}


\section{Fusion Techniques}
Exploring the state-of-the-art methodologies, we focus on feature fusion techniques that integrate texture, shape, and deep features. Fusion-based models~\cite{Liu2023,Yutong2018} that combine handcrafted features with deep learning representations have shown promise in reducing false positives and improving sensitivity, supporting their potential in clinical scenarios.

We aim to provide a comprehensive understanding of the advancements in nodule characterization. The focus is placed on the role of feature fusion techniques in enhancing diagnostic performance, particularly in terms of accuracy, sensitivity, and robustness against imaging artifacts.

\subsection{Fusion of Radiomic Features}

The following studies focus on the extraction and fusion of radiomic features to improve diagnostic accuracy, combining traditional image analysis approaches with advanced processing techniques. 

\citet{Farag2017} explored feature fusion by extracting texture descriptors (Gabor filters and Local Binary Patterns - LBP) and shape (signed distance transform fused with LBP). They showed that Gabor filters when implemented on a two-level cascaded framework with Support Vector Machines (SVM) classifiers, obtained the best performance: a mean area under the ROC curve (AUC) of 99\% and an F1-score of 97.5\%. This approach strongly encourages the premise that feature fusion, in particular with Gabor filters, could improve classification.

\citet{Shaffie2018} proposed a framework to accurately diagnose lung nodules by integrating two types of features: appearance features from a seventh-order Gibbs random field model that captures spatial heterogeneities in nodules, and geometric features defining their shape. Then a deep autoencoder classifier uses these features to distinguish between malignant and benign nodules. Evaluated with data from the LIDC, which included 727 nodules from 467 patients, the system demonstrated potential for lung cancer detection, achieving a classification accuracy of 91.20\%.

Building on this, the CAD system proposed by \citet{Shaffie2022} uses an appearance features descriptor, comprising a Histogram of Oriented Gradients, Multi-view Analytical Local Binary Patterns, and a Markov Gibbs Random Field. In addiction, employs a shape feature descriptor that includes Multi-view Peripheral Sum Curvature Scale Space, Spherical Harmonics Expansion, and a set of fundamental morphological features. Then a stacked auto-encoder followed by a soft-max classifier is applied to generate the initial malignancy probability. All of the resultant probabilities are fed to the last network that returns the diagnosis. When comparing with a previous study, they refer that the increase in the accuracy is small (from 93.97\% to 94.73\%), which is predictable, since the features used model the same nodule characteristics. However, the increase in system sensitivity from 90.48\% to 93.97\% represents a notable improvement, demonstrating that the new system, with additional features, is less affected by the segmentation process and image artifacts.

We establish that the fusion of radiomic features, particularly texture and shape descriptors, can significantly improve the performance of lung nodule classification systems. These results highlight the significance of incorporating more comprehensive features as a strategy to increase diagnostic accuracy.


\subsection{Introduction on Deep Learning Approaches}
The CNNs can automatically learn complex hierarchical features from raw image data, eliminating the need for complex manual feature engineering. Going deeper, both
\citet{Halder2020} and \citet{Gu2021} recognize the paradigm shift to deep learning-based approaches for detecting and diagnosing pulmonary nodules.

\citet{Ali2020} propose a Transferable Texture Convolutional Neural Network (CNN) for lung nodule classification, which architecture consists of three convolutional layers and an Energy Layer, omitting pooling layers to reduce trainable parameters and computational complexity. The EL preserves texture information and learns during both forward and backward propagation. The model was evaluated on the LIDC-IDRI and LUNGx Challenge datasets. The texture CNN achieved an accuracy of 96.69\% ± 0.72\% and an error rate of 3.30\% ± 0.72\% on LIDC-IDRI. Transfer learning improved accuracy on LUNGx from 86.14\% to 90.91\%. 

The study by \citet{Ali2021} evaluated the performance of Support Vector Machine and AdaBoostM2 algorithms using deep features from VGG-16, VGG-19, GoogLeNet, Inception-V3, ResNet-18, ResNet-50, ResNet-101 e InceptionResNet-V2 by identifying the optimal layers. Their results showed that SVM was more efficient for deep features as compared to AdaBoostM2. The proposed decision-level fusion technique demonstrates better results in terms of accuracy (90.46 ± 0.25\%), recovery (90.10 ± 0.44\%), and AUC (94.46 ± 0.11\%). Although it was ranked second in terms of specificity (92.56 ± 0.18\%), the deviation is notably lower compared to the Texture CNN approach~\cite{Ali2020}. Furthermore, the classification accuracy based on the simple average of the prediction scores is calculated at 89.10\%, which highlights the robustness and effectiveness of the decision fusion technique compared to other methods.


\subsection{Advanced Fusion-Based Approaches}
This present section overviews various approaches, including convolutional neural networks, feature fusion methods, attention models, and multimodal learning techniques. These studies range from the application of deep learning methods for classification to the integration of textural and visual information and other more complex approaches. As previous, the primary goal is to enhance the ability to distinguish between benign and malignant nodules, enabling earlier and more accurate diagnosis.
 
\citet{Yutong2018} gave us an algorithm, Fuse-TSD, that takes texture, shape, and deep features to automatically classify lung nodules in chest CT images. It uses a texture descriptor based on the GLCM, a Fourier shape descriptor, and a DCNN to extract features. Then classifiers, AdaBoosted Back Propagation Neural Network (BPNN), are applied for each feature and the decision is made by the fusion of the respective results. Evaluated on the LIDC-IDRI dataset, Fuse-TSD achieved an AUC of 96.65\% when nodules with a composite malignancy rate of 3 were discarded, 94.45\% when they were considered benign, and 81.24\% when they were considered malignant.

\citet{Saba2019} proposed a method for early-stage lung nodule detection, consisting of three main phases: nodule segmentation using Otsu's thresholding and morphological operations, feature extraction of geometric, texture, and deep learning features to select optimal features, and serial fusion of the optimal features for classifying nodules as malignant or benign. The study experiments with the LIDC-IDRI dataset, using Otsu's algorithm and morphological erosion for segmentation. Handcrafted geometric and texture features are combined with deep learning features extracted using a VGG-19 model. Feature optimization is performed using PCA, and the fused features are classified using multiple classifiers. Experimental results show the proposed method outperforms existing approaches, achieving an accuracy of 0.99 with fused features.

\citet{Muzammil2021} investigates different fusion approaches based on feature fusion and ensemble learning to classify lung nodules in CT scans. The authors propose two heterogeneous fusion techniques: fusion based on the average prediction score (AVG-Predict) and fusion based on majority voting (MAX-VOTE). The results showed that the MAX-VOTE technique, combining the predictions of twelve individual classifiers, achieved the highest accuracy in binary classification, with 95.59\% ± 0.27\%. While in multi classification, the SVM-FFCAT (Feature Fusion by Concatenation) method achieved superior performance, with an accuracy of 96.89\%, an AUC of 99.21\%, and a specificity of 97.70\%. These results emphasizes that fusion features with ensemble learning can significantly enhance the performance of lung nodule classification.

% 4
The CAD system presented by \citet{Yuan2022} uses a multi-branch classification network with an effective attention mechanism (3D ECA-ResNet) to extract features from 3D images of nodules, adapting dynamically to improve the extraction of key information. Structured data, such as diameter and other radiological characteristics, are transformed into a feature vector. The experimental results show that the system achieves an accuracy of 94.89\%, sensitivity of 94.91\% and an F1-score of 94.65\%, with a false positive rate of 5.55\%. The study concludes that the combination of multimodal data increases the effectiveness of the CAD system, make it more likely to be able to assist doctors in diagnosing pulmonary nodules. 

% -
The study by \citet{Liu2022} emphasizes the need to consider the temporal aspect in analyzing pulmonary nodules. It employs a Faster R-CNN to generate regions of interest and extract temporal and spatial features from lung nodule data. A 3D CNN fuses these features, and a time-modulated LSTM model (T-LSTM) analyzes trends and predicts the evolution and malignancy of lung lesions, achieving an area under the curve (AUC) of 92.71\%, surpassing traditional methods like XGBoost and RNN.

% -
\citet{Zhao2022} proposed a lung nodule detection method that integrates multi-scale feature fusion. Candidate nodules are detected using a Faster R-CNN with multi-scale features, achieving a sensitivity of 98.6\%, a 10\% improvement over single-scale models. For false positive reduction, a 3D CNN based on multi-scale fusion achieved 90.5\% sensitivity at 4 false positives per scan.

% 6
While studying the identification of COVID-19 cases, \citet{Mahmoud2022}, explored several different learning Deep-Learning Networks for thoracic image retrieval. It used two sets of data focused on the thorax: X-ray and  CT scans. Pre-trained models were used, like ResNet-50, AlexNet, and GoogleNet, as feature extractors. Similarity between images was assessed using measures such as City Block and Cosine. ResNet-50 achieved the best accuracy, reaching 99\% for positive COVID-19 cases and 98\% for negative cases in chest X-rays. 

% 7
\citet{Munoz2022} used a predictive model, such as XGBoost, based on morphological characteristics extracted from CT scans, an approach called “3D-MORPHOMICS”. Its premise is that morphological changes can be quantified and used in the diagnostic process since irregularities in the nodules are indicators of malignancy. The classification model, using only 3D-morphomic features, achieved an AUC (Area Under the ROC Curve) of 96.4\% on the NLST test set, and the combination with radiomic features resulted in even better performance, with an AUC of 97.8\% on the NLST test set and 95.8\% on the LIDC dataset.

% 10
Based on Hybrid Deep Learning models, \citet{Li2022} proposed a CAD system that integrates deep learning techniques for feature extraction and feature fusion. The system uses VGG16 and VGG19 networks with a Convolutional Block Attention Module (CBAM) to extract relevant features. These features are reduced using Principal Component Analysis (PCA) and fused via Canonical Correlation Analysis (CCA) to create effective representations. The final analysis is performed using an optimized Multiple Kernel Learning Support Vector Machine - Improved Particle Swarm Optimization (MKL-SVM-IPSO). The proposed system achieved 99.56\% accuracy, 99.3\% sensitivity, and an F1-score of 99.65\% on the LUNA16 dataset. These results demonstrate its competitiveness in reducing false positives and negatives in nodule detection.

% 11
\citet{XueLi2022} evaluated the effectiveness of fusion models in predicting axillary lymph node (ALN) metastases in breast cancer, comparing traditional radiomics models, deep learning radiomics models, and fusion models using dynamic contrast-enhanced MRI (DCE-MRI) images. The imaging data were sourced from The Cancer Imaging Archive (TCIA) via the Duke-Breast-Cancer-MRI project. Handcrafted radiomic features and deep learning features were extracted from 3062 DCE-MRI images, with feature selection performed using mutual information algorithms and recursive feature elimination. The study found that the decision fusion model, integrating radiomic and deep learning features, achieved an AUC of 0.91, outperforming traditional and deep learning models. Adding clinical features to the decision fusion model further increased the AUC to 0.93. The findings demonstrate the efficacy of fusion models in predicting ALN metastases, with the decision fusion model showing significant potential to aid clinical decision-making in early-stage breast cancer treatment.

\citet{Alksas2023} employ an approach that modifies the local ternary pattern (LTP) to use three levels instead of two and a new pattern identification algorithm to capture the heterogeneity and morphology of the nodule. Then the features were given as training data to a classification architecture based on hyper-tuned stacked generalization to classify nodules, achieving an overall accuracy of 96.17\%, with 97.14\% sensitivity and 95.33\% specificity. On the other hand, the original LBP and other classification structures resulted in lower performance when compared to the proposed approach.

% 7
\citet{Liu2023} present a novel method for classifying benign and malignant lung nodules by combining shallow visual features and deep learning features. The approach utilizes separate pipelines for feature extraction and classification. Shallow features, including texture and morphology, are extracted using statistical 3D data analysis and Haralick's texture model, while morphological features are derived from parameters such as size and shape. Support Vector Machines (SVMs) are employed to classify these extracted features. The deep learning branch uses neural architecture search to design a deep model with three sub-branches and integrates a Convolutional Block Attention Module (CBAM) for enhanced feature learning. The classification results from both shallow and deep models are fused using a weighted voting method.

% 11
\citet{Iqbal2023}, present with this study an innovative technique for classifying medical image modalities by combining visual and textural features. A pre-trained CNN extracts deep features, while manual methods like Zernike moments, Haralick features, and Global-Local Pyramid Pattern (GLPP) capture relevant textural and statistical attributes. These fused features are used to train ML classifiers such as SVM, KNN, and Decision Trees. The proposed approach outperformed standalone pre-trained CNNs, achieving 95.89\% accuracy and 96.31\% recall in modality classification.


%TODO
%%Multi-Orientation_Local_Texture_Features_for_Guided_Attention-Base
%%propuseram um módulo de atenção guiada por múltiplas orientações (MOGAM) para modelar a distribuição da textura dos nódulos em diferentes orientações. O MOGAM combina características de textura extraídas localmente (TFDs) através de um mecanismo de atenção guiada.
%Multi-Orientation Local Texture Features for Guided Attention-Based
%Shewaye et al. combinou características geométricas e de histograma para classificação de nódulos com SVM linear, regressão logística, kNN, random forest, e classificadores AdaBoost

%TODO:
%%s12890-023-02708-w.pdf
%%propuseram um algoritmo de fusão (RGD) que combina Radiomics e Graph Convolutional Networks (GCN) com modelos Deep CNN. O modelo RGD, avaliado no conjunto de dados LIDC-IDRI com validação cruzada de 10 vezes, alcançou 93.25% de precisão, 89.22% de sensibilidade, 95.82% de especificidade e 0.9629 de AUC.

%%TODO
%%Multimodal_Feature_Fusion_and_Knowledge-Driven_Learning_via_Exp
%%Avola et al. apresentou um framework baseado em expert consult para classificação de nódulos na tiróide combinando dados de ultrassom com LBP e DWT.


\section{Remarks}
We all recognize that AI has shown potential for enhancing diagnostic, reducing false positives, and optimizing the management of pulmonary nodules. However, generalization, interpretability, and and clinical integration remain major obstacles.~\cite{ArtificialIntelligence2022} It is essential that these tools are validated on larger datasets that are representative of the population to be applied and that they are integrated into clinical workflows.~\cite{Wu2024}

Standalone Deep learning approaches still miss on overcoming many challenges, for example, if the segmentation of the node is not accurate, the model may not be able to extract the features correctly, leading to inaccurate classification.~\cite{Gu2021,Shaffie2021}, allthough feature extraction show better accuracy and lower False Positive rates. Textural features, such as GLCM, are promising for differentiating nodules. The combination of feature extraction methods and neural networks optimizes diagnosis.~\cite{Mathumetha2024}

Fusion-based techniques showed potential in the classification of pulmonary nodules, addressing the limitations of autonomous feature extraction methods. By making use of supplementary information from various feature domains, these approaches increase the accuracy and reliability of the diagnosis. The studies reviewed here highlight the promise of information fusion as a critical enabler of advanced Computer Aided Diagnosis (CAD) systems, leading the way for better clinical decision-making.